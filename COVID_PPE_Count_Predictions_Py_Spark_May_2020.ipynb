{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COVID_PPE_Count_Predictions_Py_Spark_May_2020.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "# **Big Data with Spark in Google Colab**\n",
        "\n",
        "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 2.4.5 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab. \n",
        "Follow the steps to install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "source": [
        "#Set to the latest Spark version- Important\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ6NuwbvDr7H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "120be180-4899-49fa-c3d2-e8fce2676093"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "Run a local spark session to test your installation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEb4HTRwiaJx"
      },
      "source": [
        "Congrats! Your Colab is ready to run Pyspark. Let's build our first ML model. The goal of this exercise to predict the PPE equipment counts needed by considering it as the output variable, and all the other variables as input.\n",
        "\n",
        "Download the PPE Dataset and keep it somewhere on your computer. Load the dataset into your Colab directory from your local system:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrykEkZA4s6r"
      },
      "source": [
        "# Importing the basic packages plotting libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Importing the plotting libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAISFqHXf7dt"
      },
      "source": [
        "# Option 1 - exporting the file to Google colab\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm0rJX2v5AKZ"
      },
      "source": [
        "# Option 2 - importing from Google colab directory\n",
        "data = pd.read_csv(\"/content/Tarrant County COVID PPE Data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNsM_jHqrjBg"
      },
      "source": [
        "Check the dataset is uploaded correctly in the system by the following command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m606eNuQgA82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f30dc413-9dc3-490f-8c92-14db212d871b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " COVID_PPE_Count_Predictions_Py_Spark_May_2020.ipynb\n",
            " drive\n",
            " sample_data\n",
            " spark-2.4.5-bin-hadoop2.7\n",
            " spark-2.4.5-bin-hadoop2.7.tgz\n",
            "'Tarrant County COVID PPE Data.csv'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJLoAfqVv8-E"
      },
      "source": [
        "Reading the data set. Notice that we use InferSchema inside read.csv mofule. InferSchema enables us to infer automatically different data types for each column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Hxz5rxodT14"
      },
      "source": [
        "dataset = spark.read.csv('Tarrant County COVID PPE Data.csv',inferSchema=True, header =True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbAsR7UOdVnZ"
      },
      "source": [
        "Let us print look into the dataset to see the data types of each column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gok1FXWugYkE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d885f209-eb76-4542-9f32-09cc64ab4f1b"
      },
      "source": [
        "dataset.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- CaseId: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Prop of non-ER Patients: double (nullable = true)\n",
            " |-- Prop of LT50 Age Group: double (nullable = true)\n",
            " |-- Air Purity: double (nullable = true)\n",
            " |-- Living Score: double (nullable = true)\n",
            " |-- PPE Count: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN1yzAW4H9qC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "5c53ff3b-611a-4491-fb8d-7dfbb970b99f"
      },
      "source": [
        "data.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CaseId                      object\n",
              "State                       object\n",
              "Prop of non-ER Patients    float64\n",
              "Prop of LT50 Age Group     float64\n",
              "Air Purity                 float64\n",
              "Living Score               float64\n",
              "PPE Count                    int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD1WcOGC4g4M"
      },
      "source": [
        "# Computing the correlation matrix\n",
        "corr = data.corr()\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(16, 12))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# Draw the heatmap\n",
        "sns.heatmap(corr, cmap=\"YlGn\", square=True, ax = ax, annot=True, linewidth = 0.1)\n",
        "# Setting the title of the graph\n",
        "plt.title('Pearson Correlation of Features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21D9EANUvnwF"
      },
      "source": [
        "Now we can start building the ML model.Â As a first step, let us import the Vector Assembler from PySpark. \n",
        "\n",
        "Vector Assembler is a transformer that assembles all the features into one vector from multiple columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZeJ7WQCgM8g"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L9VJqsHwEGf"
      },
      "source": [
        "Next step is to create a Vector variable so as to enable to convert all the features from different columns into a single column. Let's call this new vector column as 'Attributes' in the outputCol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKSqdT9QgkfD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "85c117e9-bf78-42b1-82c2-37d18c3f377f"
      },
      "source": [
        "#Input all the features in one vector column\n",
        "assembler = VectorAssembler(inputCols=['Prop of non-ER Patients', 'Prop of LT50 Age Group', 'Air Purity', 'Living Score'], \n",
        "                            outputCol = 'Attributes')\n",
        "output = assembler.transform(dataset)\n",
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[CaseId: string, State: string, Prop of non-ER Patients: double, Prop of LT50 Age Group: double, Air Purity: double, Living Score: double, PPE Count: int, Attributes: vector]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yre6_3PYcgIC"
      },
      "source": [
        "Setting 'Attributes' as input features from all the columns and 'PPECount' as the target column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OK8heWCchO0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "cd49397d-1c3d-4a07-a1f8-619784ebc5b2"
      },
      "source": [
        "finalized_data = output.select(\"Attributes\",\"PPE Count\")\n",
        "finalized_data.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---------+\n",
            "|          Attributes|PPE Count|\n",
            "+--------------------+---------+\n",
            "|[14.96,41.76,1024...|      463|\n",
            "|[25.18,62.96,1020...|      444|\n",
            "|[5.11,39.4,1012.1...|      489|\n",
            "|[20.86,57.32,1010...|      446|\n",
            "|[10.82,37.5,1009....|      474|\n",
            "|[26.27,59.44,1012...|      444|\n",
            "|[15.89,43.96,1014...|      467|\n",
            "|[9.48,44.71,1019....|      478|\n",
            "|[14.64,45.0,1021....|      476|\n",
            "|[11.74,43.56,1015...|      478|\n",
            "|[17.99,43.72,1008...|      453|\n",
            "|[20.14,46.93,1014...|      454|\n",
            "|[24.34,73.5,1011....|      440|\n",
            "|[25.71,58.59,1012...|      451|\n",
            "|[26.19,69.34,1009...|      434|\n",
            "|[21.42,43.79,1015...|      462|\n",
            "|[18.21,45.0,1022....|      468|\n",
            "|[11.04,41.74,1022...|      477|\n",
            "|[14.45,52.75,1023...|      460|\n",
            "|[13.97,38.47,1015...|      464|\n",
            "+--------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNgFCto2wHLd"
      },
      "source": [
        "Split the training and testing data as 80%-20%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwe1VT0UNOIN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "38fbd604-6a89-42ca-f9b6-9bcd206894f1"
      },
      "source": [
        "#Split training and testing data\n",
        "train_data,test_data = finalized_data.randomSplit([0.8,0.2])\n",
        "\n",
        "# Modeling using the Linear Regression ML algorithm\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "regressor = LinearRegression(featuresCol = 'Attributes', labelCol = 'PPE Count')\n",
        "\n",
        "#Learn to fit the model from training set\n",
        "regressor = regressor.fit(train_data)\n",
        "\n",
        "#To predict the values on the testing set\n",
        "pred = regressor.evaluate(test_data)\n",
        "\n",
        "#Predict the model\n",
        "pred.predictions.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---------+------------------+\n",
            "|          Attributes|PPE Count|        prediction|\n",
            "+--------------------+---------+------------------+\n",
            "|[1.81,39.42,1026....|      491| 493.4285055354441|\n",
            "|[2.34,39.42,1028....|      490| 493.6138694112323|\n",
            "|[2.58,39.42,1028....|      489| 493.2531179164478|\n",
            "|[2.71,39.42,1026....|      489|490.98301752743583|\n",
            "|[2.8,39.64,1011.0...|      483| 489.5035503882529|\n",
            "|[2.8,39.64,1011.0...|      483| 489.5035503882529|\n",
            "|[3.0,39.64,1011.0...|      485| 489.5477761274816|\n",
            "|[3.0,39.64,1011.0...|      485| 489.5477761274816|\n",
            "|[3.2,41.31,997.67...|      490| 485.0204110275682|\n",
            "|[3.2,41.31,997.67...|      490| 485.0204110275682|\n",
            "|[3.21,38.44,1016....|      491|488.80452337873373|\n",
            "|[3.26,41.31,996.3...|      489| 484.6373597526065|\n",
            "|[3.26,41.31,996.3...|      489| 484.6373597526065|\n",
            "|[3.26,41.31,996.3...|      489| 484.6373597526065|\n",
            "|[3.38,39.64,1011....|      489| 488.6264149260246|\n",
            "|[3.38,41.31,998.7...|      489|484.90156773075734|\n",
            "|[3.4,39.64,1011.1...|      460|488.24742302640294|\n",
            "|[3.4,39.64,1011.1...|      460|488.24742302640294|\n",
            "|[3.6,35.19,1018.7...|      489|486.90727968733387|\n",
            "|[3.63,38.44,1016....|      488| 487.7647781508155|\n",
            "+--------------------+---------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3vYyp5dwOm_"
      },
      "source": [
        "We can also print the coefficient and intercept of the regression model by using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eja1BLiaTThT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "be581a8b-4224-49eb-fe13-019d7c96cefd"
      },
      "source": [
        "#coefficient of the regression model\n",
        "coeff = regressor.coefficients\n",
        "\n",
        "#X and Y intercept\n",
        "intr = regressor.intercept\n",
        "\n",
        "print (\"The coefficient of the model is : %a\" %coeff)\n",
        "print (\"The Intercept of the model is : %f\" %intr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The coefficient of the model is : DenseVector([-1.9803, -0.2331, 0.0614, -0.1563])\n",
            "The Intercept of the model is : 455.197351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYORz3Q9wTSW"
      },
      "source": [
        "# Model Evaluation\n",
        "\n",
        "Evaluting the regression model metrics by importing RegressionEvaluator module from Pyspark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qrQdEj62ptt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ef07e984-121f-480c-9176-f1fd76d2969e"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "eval = RegressionEvaluator(labelCol=\"PPE Count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "\n",
        "# Root Mean Square Error\n",
        "rmse = eval.evaluate(pred.predictions)\n",
        "print(\"RMSE: %.3f\" % rmse)\n",
        "\n",
        "# Mean Square Error\n",
        "mse = eval.evaluate(pred.predictions, {eval.metricName: \"mse\"})\n",
        "print(\"MSE: %.3f\" % mse)\n",
        "\n",
        "# Mean Absolute Error\n",
        "mae = eval.evaluate(pred.predictions, {eval.metricName: \"mae\"})\n",
        "print(\"MAE: %.3f\" % mae)\n",
        "\n",
        "# r2 - coefficient of determination\n",
        "r2 = eval.evaluate(pred.predictions, {eval.metricName: \"r2\"})\n",
        "print(\"r2: %.3f\" %r2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 4.560\n",
            "MSE: 20.796\n",
            "MAE: 3.614\n",
            "r2: 0.929\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}